                       :-) GROMACS - gmx grompp, 2020 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2018, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx grompp, version 2020
Executable:   /home/mabraham/git/bioexcel-exascale-co-design-benchmarks/GROMACS/nonbonded_benchmark/gromacs_source_code/bin/gmx
Data prefix:  /home/mabraham/git/bioexcel-exascale-co-design-benchmarks/GROMACS/nonbonded_benchmark/gromacs_source_code (source tree)
Working dir:  /home/mabraham/git/bioexcel-exascale-co-design-benchmarks/GROMACS/nonbonded_benchmark/gromacs_regressiontests/complex/nbnxn-ljpme-LB-geometric
Command line:
  gmx grompp -f ./grompp.mdp -c ./conf -r ./conf -p ./topol -maxwarn 10

Ignoring obsolete mdp entry 'andersen_seed'
Ignoring obsolete mdp entry 'dihre'
Ignoring obsolete mdp entry 'dihre-fc'
Ignoring obsolete mdp entry 'optimize_fft'
Ignoring obsolete mdp entry 'rlistlong'
Ignoring obsolete mdp entry 'gb_algorithm'
Ignoring obsolete mdp entry 'nstgbradii'
Ignoring obsolete mdp entry 'rgbradii'
Ignoring obsolete mdp entry 'gb_epsilon_solvent'
Ignoring obsolete mdp entry 'gb_saltconc'
Ignoring obsolete mdp entry 'gb_obc_alpha'
Ignoring obsolete mdp entry 'gb_obc_beta'
Ignoring obsolete mdp entry 'gb_obc_gamma'
Ignoring obsolete mdp entry 'gb_dielectric_offset'
Ignoring obsolete mdp entry 'sa_algorithm'
Ignoring obsolete mdp entry 'sa_surface_tension'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file ./grompp.mdp]:
  With Verlet lists the optimal nstlist is >= 10, with GPUs >= 20. Note
  that with the Verlet scheme, nstlist has no effect on the accuracy of
  your simulation.


NOTE 2 [file ./grompp.mdp]:
  nstcomm < nstcalcenergy defeats the purpose of nstcalcenergy, setting
  nstcomm to nstcalcenergy

Generated 231 of the 231 non-bonded parameter combinations
Generating 1-4 interactions: fudge = 0.5
Generated 231 of the 231 1-4 parameter combinations
Excluding 3 bonded neighbours molecule type 'POPC'
Excluding 2 bonded neighbours molecule type 'SOL'
Removing all charge groups because cutoff-scheme=Verlet
Number of degrees of freedom in T-Coupling group System is 108.00

NOTE 3 [file ./grompp.mdp]:
  You are using geometric combination rules in LJ-PME, but your non-bonded
  C6 parameters do not follow these rules. This will introduce very small
  errors in the forces and energies in your simulations. If your system is
  homogeneous, consider using dispersion correction for the total energy
  and pressure.

Estimate for the relative computational load of the PME mesh part: 1.00

NOTE 4 [file ./grompp.mdp]:
  The optimal PME mesh load for parallel simulations is below 0.5
  and for highly parallel simulations between 0.25 and 0.33,
  for higher performance, increase the cut-off and the PME grid spacing.



There were 4 notes

Thanx for Using GROMACS - Have a Nice Day

