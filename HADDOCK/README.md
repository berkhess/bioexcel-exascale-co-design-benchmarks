# Background
The vast network of the interactome involves hundreds of thousands of protein-protein and other biomolecular complex interactions. 
Malfunctions in this network are responsible for a plethora of diseases, which highlights the importance of understanding how it works on a deep level. 
Adding structural information to such an intricate network comes with two challenges; the generation of hundreds of thousands of independent docking runs and an all-vs-all docking of the network components for further analysis. 
This task will generate a gargantuan number of files and will only be possible by taking advantage of exascale computing resources both for processing and I/O handling. 
This benchmark is related to our BioExcel project on high throughput modelling of interactomes.

# Description

Here we present a I/O benchmark consisting of 1,000,000 files â€“ docking models in PDB format (plain text file) - generated by HADDOCK.
Each of these files contains in its header various energetic components that have been calculated by HADDOCK. 

```REMARK FILENAME="1ATN_1061.pdb0"
REMARK ===============================================================
REMARK HADDOCK run for 1ATN
REMARK initial structure: 1ATN_1.pdb
REMARK final NOE weights: unambig 50 amb: 50
REMARK ===============================================================
REMARK            total,bonds,angles,improper,dihe,vdw,elec,air,cdih,coup,rdcs,vean,dani,xpcs,rg
REMARK energies: 642.296, 0, 0, 0, 0, 4.80171, -0.512644, 638.007, 0, 0, 0, 0, 0, 0, 0
REMARK ===============================================================
REMARK            bonds,angles,impropers,dihe,air,cdih,coup,rdcs,vean,dani,xpcs
REMARK rms-dev.: 0,0,0,0,1.07224,0,0, 0, 0, 0, 0
REMARK ===============================================================
REMARK               air,cdih,coup,rdcs,vean,dani,xpcs
REMARK               >0.3,>5,>1,>0,>5,>0.2,>0.2
REMARK violations.: 8, 0, 0, 0, 0, 0, 0
REMARK ===============================================================
REMARK                        CVpartition#,violations,rms
REMARK AIRs cross-validation: 0, 0, 0
REMARK ===============================================================
REMARK NCS energy: 0
REMARK ===============================================================
REMARK Symmetry energy: 0
REMARK ===============================================================
REMARK Membrane restraining energy: 0
REMARK ===============================================================
REMARK Local cross-correlation:  0.0000
REMARK ===============================================================
REMARK Desolvation energy: 2.18009
REMARK Internal energy free molecules: 17670.2
REMARK Internal energy complex: 17670.2
REMARK Binding energy: 6.46916
REMARK ===============================================================
REMARK buried surface area: 1319.21
REMARK ===============================================================
REMARK water - chain_1: 0 0 0
REMARK water - chain_2: 0 0 0
REMARK ===============================================================
REMARK water - water: 0 0 0
REMARK ===============================================================
REMARK DATE:25-Dec-2018  19:07:53       created by user: enm009
```

The benchmark is composed of the results of applying HADDOCK to the cases of the Protein-Protein Docking Benchmark v5 (BM5), which is a well-accepted benchmark in the protein docking community.   Due to the nature of the BM5 benchmark, the PDB files are different in length as they represent different molecular structures.  Files are placed in a single directory and named according to the BM5 they originated from, docking procedure used, docking stage and model number, example; 1BVN_ti5-it1_167.pdb and 1AK4_cm-it0_1601.pdb.

The HADDOCK score itself is a linear combination of the extracted energetic components, weighted according to its stage (rigid-body, semi-flexible or water).
For more information about the scoring, please refer to the [HADDOCK Manual](http://www.bonvinlab.org/software/haddock2.2/scoring/).

# Download

The datasets (4 of those containing each 250k models) can be downloaded from [DOI:10.5281/zenodo.3271425](https://doi.org/10.5281/zenodo.3271425)


This script can be used for downloading with streamed unpacking:

```
for n in 01 02 03 04 ; do 
  (curl https://zenodo.org/record/3271425/files/dataset-$n.tar.gz?download=1 | tar zxf -) & done
```

The above can be sensitive to network connectivity issues, for more reliable download try:

```
curl --retry 3 -fO https://zenodo.org/record/3271425/files/dataset-0[1-4].tar.gz?download=1

md5sum -c - <<EOF
e81b134c8ddd035985a63301e07ac7ed  dataset-01.tar.gz
689efdee5a1374f0f1ac1ef52beffe39  dataset-02.tar.gz
de9db417622dc43472da7ac88954d2ef  dataset-03.tar.gz
174c60b6b28205b21256326f6b580f95  dataset-04.tar.gz
EOF

for x in dataset-*.tar.gz ; do tar zxf $x ; done
```


# Requirements

* Python 3.6.x
* ~ 80 Gb (Compressed, 20Gb each)
* ~ 375 Gb (Uncompressed, 93Gb each)

# Usage

Make sure the datasets are unpacked and create first a file list containing all models to be analysed.
For example to analyse 250k models:

```bash 
$ i=250000 ; find dataset-*/. -type f | head -n $i > dataset-250k.file
```
And to analyse 1 million models:

```bash 
$ i=1000000 ; find dataset-*/. -type f | head -n $i > dataset-1M.file
```


Execute main code:

```bash
$ python3 calc-batch-hs.py dataset-250k.file
```


# Scenario
 
 Processing these files quickly without straining the system will require extremely efficient I/O solutions.
In order to evaluate this scenario we created a python script named `calc-batch-hs.py` that executes the following tasks:
1. Extract the energy terms from the header of the files
2. Calculate the HADDOCK score and 
3. Write a sorted list which represents the final ranking. 


# Timing examples

To exemplify the volume of I/O that is executed during HADDOCK simulations, we wrote a first prototype in Python to extract the energetic terms from the header of the different files and to calculate the HADDOCK score. The timing tests were executed using the Linux native time command on two setups:

1) Linux server, running Scientific Linux 6.3 - 8x2.8 GHz AMD 6320, 64 GB memory, with a 10TB RAID6 system (LSI MegaRAID SAS 9270-8i - x8 PCI Express 3.0 host interface; 12 Hitachi Ultrastar C10K900 900GB 2.5" SAS 6Gb/s 10KRPM 64MB); 

2) Linux server running CentOS 7 - 8x2.20 GHz Intel Xeon E5-2650, 128GB memory (8x16GB DDR4-2400MHz ECC Registered DIMM - Dual Rank) and 1 SSD 960GB disk (Kingston HyperX Predator 960 GB I - PCI Express interface with NVMe protocol). 


__Timing results [h : m : s] and I/O events on 10TB RAID6 system:__

| Subset | User  | System  | Real  | Filesystem<br>input I/O events | Filesystem<br>output I/O events |
|--:|---|---|---|--:|--:|
| 250'000 | 00:00:48  | 00:00:20 | 00:23:41 | 6,939,112 | 17,624 |
| 500'000 | 00:01:23  | 00:00:30 | 00:33:34 | 8,240,824 | 35,256 |
| 750'000 | 00:02:31  | 00:01:05 | 01:25:32 | 22,478,552 | 52,832 |
| 1'000'000 | 00:03:28 | 00:01:36 | 02:15:42 | 32,265,832 | 70,472 |


__Timing results [h : m : s] and I/O events on 960GB SSD PCI disk:__

| Subset | User  | System  | Real  | Filesystem<br>input I/O events | Filesystem<br>output I/O events |
|--:|---|---|---|--:|--:|
| 250'000 | 00:00:22  | 00:00:10 | 00:01:02 | 8,011,504 | 28,336 |
| 500'000 | 00:00:34  | 00:00:12 | 00:01:18 | 8,003,000 | 56,664 |
| 750'000 | 00:00:46  | 00:00:16 | 00:01:33 | 9,963,880 | 85,000 |
| 1'000'000 | 00:00:50 | 00:00:15 | 00:01:22 | 8,003,016 | 113,328 |


***

This benchmark was created by Rodrigo Honorato and Alexandre Bonvin @ Utrecht University // [BonvinLab](http://www.bonvinlab.org)
